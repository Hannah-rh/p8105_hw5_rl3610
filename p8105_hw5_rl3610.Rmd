---
title: "p8105_hw5_rl3610"
author: "Ruohan Lyu"
date: "2025-11-11"
output: github_document
---

```{r, include = FALSE, message = FALSE, warning = FALSE}
library(tidyverse)

knitr::opts_chunk$set(
	echo = TRUE,
	warning = FALSE,
  fig.width = 6,
  fig.asp = .6,
  out.width = "90%"
)

theme_set(theme_minimal() + theme(legend.position = "bottom"))

options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)

scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d
```

# Problem 1

### Create a funtion

```{r}
bday_sim = function(n_room) {
  
  birthdays = sample(1:365, n_room, replace = TRUE)

  repeated_bday = length(unique(birthdays)) < n_room

  repeated_bday
  
}
```

### Simulation

```{r}
bday_sim_results = 
  expand_grid(
    bdays = 2:50, 
    iter = 1:10000
  ) |> 
  mutate(
    result = map_lgl(bdays, bday_sim)
  ) |> 
  group_by(
    bdays
  ) |> 
  summarize(
    prob_repeat = mean(result)
  )
```

### Make the plot

```{r}
bday_sim_results |> 
  ggplot(aes(x = bdays, y = prob_repeat)) + 
  geom_point() + 
  geom_line() +
  labs(
    title = "Probability of Shared Birthday by Group Size",
    x = "Group Size",
    y = "Probability of Shared Birthday",
    caption = "Based on 10,000 simulations per group size"
  ) +
  scale_y_continuous(labels = scales::percent_format()) +
  theme_minimal()
```

The plot shows that the probability of shared birthday rises quickly as the number of people in the room increases. When the group size is small(fewer than 10 people), the probability of a shared birthday is very low. Around a group size of 23, the probability reaches about 50%, and by the time the group reaches around 50 people, the probability is close to 100%.


# Problem 2

### Create a funtion

```{r}
t_test_sim = function(mu) {

  x = rnorm(n = 30, mean = mu, sd = 5)

  test_result = t.test(x, mu = 0)
  
  tidy_results = broom::tidy(test_result)
  
  tibble(
    mu_hat = pull(tidy_results, estimate),
    p_value = pull(tidy_results, p.value)
  )
  
}
```

### Simulation

```{r}
power_sim_results_df = 
  expand_grid(
    true_mu = c(0:6),
    iter = 1:5000
  ) |> 
  mutate(
    results = map(true_mu, t_test_sim)
  ) |> 
  unnest(results) |> 
  mutate(
    reject_null = p_value < 0.05
  )
```

### Plot 1: Power vs. Effect Size

```{r}
power1_summary_df = 
  power_sim_results_df |> 
  group_by(true_mu) |> 
  summarize(
    power = mean(reject_null)
  )

power1_summary_df |> 
  ggplot(aes(x = true_mu, y = power)) + 
  geom_point() + 
  geom_line() +
  labs(
    title = "Statistical Power vs. Effect Size",
    x = "True Effect Size (μ)",
    y = "Power (Probability of Rejecting H0)",
    caption = "n = 30, σ = 5, α = 0.05, 5000 simulations per effect size"
  ) +
  scale_y_continuous(labels = scales::percent_format()) +
  theme_minimal()

power1_summary_df
```

The simulation results demonstrate a strong positive association between effect size and statistical power. As μ increases, the power of the t-test rises dramatically from approximately 5% to nearly 100%, showing that larger effect sizes are detected with substantially higher probability given a sample size of 30.


### Plot 2: Average Estimates vs. True Effect Size

```{r}
power2_summary_df = 
  power_sim_results_df |> 
  group_by(true_mu) |> 
  summarize(
    avg_mu_hat = mean(mu_hat),
    avg_mu_hat_rejected = mean(mu_hat[reject_null])
  )

power2_summary_df |> 
  select(true_mu, avg_mu_hat, avg_mu_hat_rejected) |> 
  pivot_longer(
    cols = c(avg_mu_hat, avg_mu_hat_rejected),
    names_to = "estimate_type",
    values_to = "mean_estimate"
  ) |> 
  mutate(
    estimate_type = case_when(
      estimate_type == "avg_mu_hat" ~ "All Samples",
      estimate_type == "avg_mu_hat_rejected" ~ "Only When H₀ Rejected"
    )
  ) |> 
  ggplot(aes(x = true_mu, y = mean_estimate, color = estimate_type)) + 
  geom_point() + 
  geom_line() +
  geom_abline(slope = 1, intercept = 0, linetype = "dashed", alpha = 0.5) +
  labs(
    title = "Average Estimates vs. True Effect Size",
    x = "True Effect Size (μ)",
    y = "Average Estimate (μ̂)",
    color = "Sample Type",
    caption = "Dashed line represents perfect estimation (y = x)"
  ) +
  theme_minimal()

power2_summary_df
```

No, the sample average of μ̂ across tests for which the null is rejected is not approximately equal to the true value of μ, especially for small effects. This overestimation occurs because we only reject the null hypothesis when sampling variation produces unusually large estimates. The bias diminishes for larger true effects, where significance is almost guaranteed.


p8105_hw5_rl3610
================
Ruohan Lyu
2025-11-11

# Problem 1

### Create a funtion

``` r
bday_sim = function(n_room) {
  
  birthdays = sample(1:365, n_room, replace = TRUE)

  repeated_bday = length(unique(birthdays)) < n_room

  repeated_bday
  
}
```

### Simulation

``` r
bday_sim_results = 
  expand_grid(
    bdays = 2:50, 
    iter = 1:10000
  ) |> 
  mutate(
    result = map_lgl(bdays, bday_sim)
  ) |> 
  group_by(
    bdays
  ) |> 
  summarize(
    prob_repeat = mean(result)
  )
```

### Make the plot

``` r
bday_sim_results |> 
  ggplot(aes(x = bdays, y = prob_repeat)) + 
  geom_point() + 
  geom_line() +
  labs(
    title = "Probability of Shared Birthday by Group Size",
    x = "Group Size",
    y = "Probability of Shared Birthday",
    caption = "Based on 10,000 simulations per group size"
  ) +
  scale_y_continuous(labels = scales::percent_format()) +
  theme_minimal()
```

<img src="p8105_hw5_rl3610_files/figure-gfm/unnamed-chunk-4-1.png" width="90%" />

The plot shows that the probability of shared birthday rises quickly as
the number of people in the room increases. When the group size is
small(fewer than 10 people), the probability of a shared birthday is
very low. Around a group size of 23, the probability reaches about 50%,
and by the time the group reaches around 50 people, the probability is
close to 100%.

# Problem 2

### Create a funtion

``` r
t_test_sim = function(mu) {

  x = rnorm(n = 30, mean = mu, sd = 5)

  test_result = t.test(x, mu = 0)
  
  tidy_results = broom::tidy(test_result)
  
  tibble(
    mu_hat = pull(tidy_results, estimate),
    p_value = pull(tidy_results, p.value)
  )
  
}
```

### Simulation

``` r
power_sim_results_df = 
  expand_grid(
    true_mu = c(0:6),
    iter = 1:5000
  ) |> 
  mutate(
    results = map(true_mu, t_test_sim)
  ) |> 
  unnest(results) |> 
  mutate(
    reject_null = p_value < 0.05
  )
```

### Plot 1: Power vs. Effect Size

``` r
power1_summary_df = 
  power_sim_results_df |> 
  group_by(true_mu) |> 
  summarize(
    power = mean(reject_null)
  )

power1_summary_df |> 
  ggplot(aes(x = true_mu, y = power)) + 
  geom_point() + 
  geom_line() +
  labs(
    title = "Statistical Power vs. Effect Size",
    x = "True Effect Size (μ)",
    y = "Power (Probability of Rejecting H0)",
    caption = "n = 30, σ = 5, α = 0.05, 5000 simulations per effect size"
  ) +
  scale_y_continuous(labels = scales::percent_format()) +
  theme_minimal()
```

<img src="p8105_hw5_rl3610_files/figure-gfm/unnamed-chunk-7-1.png" width="90%" />

``` r
power1_summary_df
```

    ## # A tibble: 7 × 2
    ##   true_mu  power
    ##     <int>  <dbl>
    ## 1       0 0.0464
    ## 2       1 0.192 
    ## 3       2 0.562 
    ## 4       3 0.889 
    ## 5       4 0.987 
    ## 6       5 1     
    ## 7       6 1

The simulation results demonstrate a strong positive association between
effect size and statistical power. As μ increases, the power of the
t-test rises dramatically from approximately 5% to nearly 100%, showing
that larger effect sizes are detected with substantially higher
probability given a sample size of 30.

### Plot 2: Average Estimates vs. True Effect Size

``` r
power2_summary_df = 
  power_sim_results_df |> 
  group_by(true_mu) |> 
  summarize(
    avg_mu_hat = mean(mu_hat),
    avg_mu_hat_rejected = mean(mu_hat[reject_null])
  )

power2_summary_df |> 
  select(true_mu, avg_mu_hat, avg_mu_hat_rejected) |> 
  pivot_longer(
    cols = c(avg_mu_hat, avg_mu_hat_rejected),
    names_to = "estimate_type",
    values_to = "mean_estimate"
  ) |> 
  mutate(
    estimate_type = case_when(
      estimate_type == "avg_mu_hat" ~ "All Samples",
      estimate_type == "avg_mu_hat_rejected" ~ "Only When H₀ Rejected"
    )
  ) |> 
  ggplot(aes(x = true_mu, y = mean_estimate, color = estimate_type)) + 
  geom_point() + 
  geom_line() +
  geom_abline(slope = 1, intercept = 0, linetype = "dashed", alpha = 0.5) +
  labs(
    title = "Average Estimates vs. True Effect Size",
    x = "True Effect Size (μ)",
    y = "Average Estimate (μ̂)",
    color = "Sample Type",
    caption = "Dashed line represents perfect estimation (y = x)"
  ) +
  theme_minimal()
```

<img src="p8105_hw5_rl3610_files/figure-gfm/unnamed-chunk-8-1.png" width="90%" />

``` r
power2_summary_df
```

    ## # A tibble: 7 × 3
    ##   true_mu avg_mu_hat avg_mu_hat_rejected
    ##     <int>      <dbl>               <dbl>
    ## 1       0    0.00353              0.0620
    ## 2       1    1.02                 2.27  
    ## 3       2    2.01                 2.63  
    ## 4       3    3.02                 3.21  
    ## 5       4    4.01                 4.04  
    ## 6       5    4.98                 4.98  
    ## 7       6    6.01                 6.01

No, the sample average of μ̂ across tests for which the null is rejected
is not approximately equal to the true value of μ, especially for small
effects. This overestimation occurs because we only reject the null
hypothesis when sampling variation produces unusually large estimates.
The bias diminishes for larger true effects, where significance is
almost guaranteed.
